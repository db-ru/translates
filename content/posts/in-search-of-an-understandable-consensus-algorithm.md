---
title: "В поисках понятного алгоритма консенсуса"
summary: "Raft - это алгоритм консенсуса для управления реплицированным логом.
Он дает результат, эквивалентный (мульти) Paxos, и он так же эффективен, но
его структура отличается от Paxos; это делает Raft более понятным, чем Paxos, а
также обеспечивает лучшую основу для построения практических систем. Чтобы
повысить понятность, Raft разделяет ключевые элементы консенсуса, такие как
выбор лидера, репликация журнала и безопасность, и обеспечивает более высокую
степень согласованности для сокращения числа состояний, которые необходимо
учитывать. Результаты пользовательского исследования показывают, что Raft легче
учиться, чем Paxos. Raft также включает в себя новый механизм изменения членства
в кластере, который использует перекрывающееся большинство для обеспечения
безопасности."
date: 2020-05-16T19:22:56+03:00
draft: true
toc: true
categories:
- paxos
- consencus
- raft
- distributed
---

{{< param Summary >}}

Этот технический отчет является расширенной версией [[32]]; дополнительный
материал отмечен серой полосой на полях. Опубликовано 20 мая 2014 г.

## 0. Оригинал {#origin}

Diego Ongaro and John Ousterhout, [In Search of an Understandable Consensus
Algorithm (Extended Version)](https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf)

## 1. Введение {#introduction}

Консенсусные алгоритмы позволяют совокупности машин работать как единая группа,
которая может пережить сбои (падения) некоторых ее членов. Поэтому они играют
ключевую роль в создании надежных крупномасштабных программных систем. Paxos
[[15], [16]] доминировал в обсуждении алгоритмов консенсуса в течение последнего
десятилетия: большинство реализаций консенсуса основаны на Paxos или под его
влиянием, и Paxos стал основным средством, используемым для обучения студентов
консенсусу.

К сожалению, Paxos довольно сложно понять, даже несмотря на многочисленные
попытки сделать его более доступным. Более того, его архитектура требует сложных
изменений для поддержки практических систем. В результате, как разработчики
систем, так и студенты борются с Paxos.

После того, как мы сами боролись с Paxos, мы решили найти новый алгоритм поиска
консенсуса, который мог бы обеспечить лучшую основу для построения систем и
обучения. Наш подход был необычным в том смысле, что нашей главной целью была
понятность: можем ли мы определить алгоритм консенсуса для практических систем
и описать его так, чтобы его было значительно легче изучить, чем Paxos? Более
того, мы хотели, чтобы алгоритм способствовать развитию интуиции, которые
необходимы для разработчиков систем. Важно было не только, чтобы алгоритм
работал, но и чтобы было понятно, почему он работает.

Результатом этой работы является алгоритм поиска согласованного состояния
(консенсуса) под названием Raft. При разработке Raft мы применяли специальные
методы для улучшения понимания, включая декомпозицию (Raft разделяет выбор
лидера, журнал репликации и безопасность) и сокращение пространства состояний
(по сравнению с Paxos, Raft уменьшает степень недетерминированности и то, как
серверы могут быть несовместимы друг с другом). Пользовательское исследование с
43 студентами в двух университетах показывает, что Raft значительно легче
понять, чем Paxos: после изучения обоих алгоритмов 33 из этих студентов смогли
ответить на вопросы о Raft лучше, чем на вопросы о Paxos.

Raft во многом похож на существующие алгоритмы консенсуса (в частности,
Viewstamped Replication Оки и Лискова [[29], [22]]), но у него есть несколько
нововведений:

- **Сильный лидер**: Raft использует более сильную форму лидерства, чем другие
  алгоритмы консенсуса. Например, записи журнала передаются только от лидера
  к другим серверам. Это упрощает управление журналом репликации и
  облегчает понимание Raft.

- **Выбор лидера**: Raft использует случайные таймеры для выбора лидеров. Это
  добавляет лишь небольшое количество работы к сигналам сихнронизации
  (heartbeat), которые и так требуются для любого алгоритма консенсуса. Но при
  этом упрощаются и ускоряеются разрешения конфликтов.

- **Изменения в членстве**: Механизм Raft для изменения набора серверов в кластере
  использует новый совместный консенсусный подход, при котором большинство двух
  разных конфигураций перекрываются во время переходов. Это позволяет кластеру
  нормально работать во время изменений конфигурации.

Мы считаем, что Raft превосходит Paxos и другие алгоритмы консенсуса, как в
образовательных целях, так и в качестве основы для реализации. Он проще и
понятнее, чем другие алгоритмы; он описан достаточно полно, чтобы удовлетворить
потребности практической системы; он имеет несколько реализаций с открытым
исходным кодом и используется несколькими компаниями; его защитные свойства были
официально определены и доказаны; и его эффективность сопоставима с другими
алгоритмами.

Оставшиеся части статьи знакомят с проблемой реплицированных конечных автоматов
(*replicated state machine, RSM*) ([раздел 2]({{<relref "#replicated-state-machines">}})),
обсуждают сильные и слабые стороны Paxos ([раздел 3]({{<relref "#what-is-wrong-with-paxos">}})),
описывают наш обобщенный подход к пониманиемости (раздел 4),
представляют алгоритм консенсуса Raft (разделы 5–8), дают оценку Raft (Раздел 9)
и обсуждают связанную с этим работу (Раздел 10).

## 2. Реплицированный конечный автомат {#replicated-state-machines}

Алгоритмы консенсуса обычно возникают в контексте реплицированных конечных
автоматов [[37]]. При таком подходе конечные автоматы на нескольких серверах
вычисляют идентичные копии одного и того же состояния и могут продолжать
работать, даже если некоторые из серверов не работают.

Реплицированные конечные автоматы используются для решения различных проблем
отказоустойчивости в распределенных системах. Например, в больших масштабируемых
системах, в которых один лидер в кластере, таких как GFS [[8]], HDFS [[38]] и
RAMCloud [[33]], обычно используется отдельный реплицированный конечный автомат
для управления выбором лидера и хранения информации о конфигурации, которая
должна сохраниться при падении лидера. Примеры реплицированных конечных
автоматов включают Chubby [[2]] и ZooKeeper [[11]].

Реплицированные конечные автоматы обычно реализуются с использованием
журнала репликации, как показано на [рисунке 1]({{< relref "#rsm_arch" >}}). Каждый сервер хранит
журнал, содержащий серию команд, которые его конечный автомат выполняет по
порядку. Каждый журнал содержит одинаковые команды в одинаковом порядке, поэтому
каждый конечный автомат обрабатывает одну и ту же последовательность команд.
Поскольку конечные автоматы являются детерминированными, каждый вычисляет одно и
то же состояние и одну и ту же последовательность выходных данных.

{{< figure_wid id="rsm_arch" src="/img/raft/rsm_arch.png" title="Рисунок 1. Архитектура реплицированного конечного автомата." caption="Алгоритм консенсуса (consensus module) управляет журналом репликации (Log), содержащим команды конечного автомата от клиентов (clients). Конечные автоматы обрабатывают идентичные последовательности команд из журналов, поэтому они выдают одинаковые выходные данные." >}}

Поддержка лога репликации в согласованном состоянии - задача алгоритма
консенсуса. Модуль консенсуса на сервере получает команды от клиентов и
добавляет их в свой журнал. Он общается с модулями консенсуса на других
серверах, чтобы гарантировать, что каждый журнал в конечном итоге содержит
одинаковые запросы в одинаковом порядке, даже если некоторые серверы выходят из
строя. Как только команды отреплицированы, конечный автомат каждого сервера
обрабатывает их в порядке ведения журнала, а выходные данные возвращаются
клиентам. В результате серверы образуют единый высоконадежный конечный автомат.

Алгоритмы консенсуса на практике обычно имеют следующие свойства:

- Они гарантируют **безопасность** (никогда не возвращая неверный результат) при
  всех невизантийских условиях, включая сетевые задержки, разбиения и потери
  пакетов, дублирование и переупорядочение.

- Они полностью функциональны (**доступны**) пока большинство серверов
  работает и может связываться друг с другом и с клиентами. Таким образом,
  типичный кластер из пяти серверов может терпеть сбой любых двух серверов.
  Предполагается, что серверы выходят из строя, если они остановлены; они
  могут позже восстановиться из состояния на стабильном хранилище и вернуться
  в кластер.

- Они не зависят от времени для обеспечения согласованности журналов:
  неисправные часы и экстремальные задержки сообщений могут в худшем случае
  вызвать проблемы с доступностью.

- В общем случае команда может завершиться, как только бо́льшая часть кластера
  ответит на один раунд удаленных вызовов процедур; меньшинство медленных
  серверов не должно влиять на общую производительность системы.

## 3. Что не так с Paxos? {#what-is-wrong-with-paxos}

За последние десять лет протокол Paxos Лесли Лэмпорта [15] стал почти синонимом
консенсуса: этот протокол чаще всего преподается на курсах, и большинство
реализаций консенсуса используют его в качестве отправной точки. Сначала Paxos
определяет протокол, способный достичь согласия (консенсуса) по одному решению,
например, по записи в журнале репликации. Мы называем это подмножество как
одно-указный Paxos (*single-decree Paxos*). Затем Paxos объединяет несколько
экземпляров этого протокола для облегчения серии решений, таких как журнал
(*multi-Paxos*). Paxos обеспечивает безопасность (safety) и жизнеспособность
(liveness), а также поддерживает изменения в составе кластера. Его правильность
доказана, и в обычном случае он эффективен.

К сожалению, у Paxos есть два существенных недостатка. Первый недостаток в том,
что Paxos исключительно сложен для понимания. Полное объяснение [[15]]
- общеизвестно непрозрачно; немногие люди понимают его, и только с большими
усилиями. В результате было несколько попыток объяснить Paxos в более простых
терминах [[16], [20], [21]]. Эти объяснения сосредоточены на подмножестве
одно-указного Paxos (single-decree Paxos), но и они всё ещё остаются сложными.
В неофициальном опросе участников на NSDI 2012, мы обнаружили мало людей,
которым было бы удобно с Paxos, даже среди опытных исследователей. Мы сами
боролись с Paxos; мы не могли понять протокол полностью до тех пор, пока не
прочитали несколько упрощенных объяснений и не разработали собственный
альтернативный протокол - процесс, который занял почти год.

Мы предполагаем, что непрозрачность Paxos исходит из выбора в качестве основы
одно-указного подмножества (single-decree). Одно-указный Paxos непрозрачный и
хрупкий: он разделен на две стадии, которые не имеют простых наглядных
объяснений и не могут быть поняты независимо. Из-за этого трудно интуитивно
понять, почему этот протокол работает. Правила композиции multi-Paxos
добавляют значительную дополнительную сложность и хрупкость. Мы считаем, что
общая проблема достижения консенсуса по нескольким решениям (таким как: журнал
вместо одной записи) может быть декомпозирована другими способами, которые
являются более точными и очевидными.

Вторая проблема Paxos заключается в том, что он не обеспечивает хорошую основу
для создания практических реализаций. Одна из причин заключается в том, что не
существует широко согласованного алгоритма для multi-Paxos. Описания Лампорта в
основном касаются single-decree Paxos; он набросал возможные подходы к
multi-Paxos, но многие детали отсутствуют. Было несколько попыток
конкретизировать и оптимизировать Paxos, такие как [[26]], [[39]] и [[13]],
но они отличаются друг от друга и от эскизов Лампорта. В таких системах,
как Chubby [[4]], реализованы алгоритмы, подобные Paxos, но в большинстве случаев их
подробности не публикуются.

Помимо этого, архитектура Paxos плоха для построения практических систем; это
еще одно следствие single-decree декомпозиции. Например, нет особого
преимущества, если вы выбираете набор записей журнала независимо, а затем
объединяете их в последовательный журнал; это только добавляет сложности. 
Проще и эффективнее спроектировать систему вокруг журнала, в которой новые
записи добавляются последовательно в ограниченном порядке. Другая проблема
заключается в том, что Paxos использует симметричный одноранговый (peer-to-peer)
подход в своей основе (хотя в конечном итоге предлагается слабая форма
лидерства в качестве оптимизации производительности). Это имеет смысл в
упрощенном мире, где будет принято только одно решение, и лишь немногие
системы на практике используют этот подход. Если необходимо принять ряд решений,
проще и быстрее сначала выбрать лидера, а затем назначить лидера для координации
решений.

В результате, практические системы мало похожи на Paxos. Каждая реализация
начинается с Paxos, обнаруживает трудности в ее реализации, а затем
разрабатывает существенно другую архитектуру. Это отнимает много времени и
подвержено ошибкам, а трудности понимания Paxos усугубляют проблему.
Формулировка Paxos может быть хорошей для доказательства теорем о ее
правильности, но реальные реализации настолько отличаются от Paxos, что
доказательства имеют небольшую ценность. Следующий комментарий от разработчиков
Chubby является типичным:

> Существуют значительные пробелы между описанием алгоритма Paxos и
> потребностями реальной системы... окончательная система будет основана на
> недоказанном протоколе [[4]].

Из-за этих проблем мы пришли к выводу, что Paxos не обеспечивает хорошей основы
ни для разработки систем, ни для образования. Учитывая важность консенсуса в
больших масштабируемых системах, мы решили посмотреть, сможем ли мы
разработать альтернативный алгоритм консенсуса с лучшими свойствами, чем у
Paxos.

## 4. Проектирование для понятности {#designing-for-understandability}

У нас было несколько целей при разработке Raft: он должен обеспечить полную и
практическую основу для разработки систем, чтобы значительно сократить объем
проектных работ, требуемых от разработчиков; он должен быть безопасным при любых
условиях и доступным при обычных условиях эксплуатации; и он должен быть
эффективным для обычных операций. Но нашей самой важной целью — и самой сложной
задачей - была *понятность*. Важно, чтобы алгоритмы был понятен для большой
аудитории без особых сложностей. Дополнительно, алгоритм должен быть
интуитивным, чтобы разработчики могли создавать расширения, которые неизбежны
в реальных реализациях.

В дизайне Raft было много моментов, когда нам приходилось выбирать среди
альтернативных подходов. В этих ситуациях мы оценивали альтернативы на основе
понятности: насколько сложно объяснить каждую альтернативу (например, насколько
сложным является её пространство состояний и имеет ли она неочевидные
моменты?) И насколько легко читателю будет полностью понять подход и его
суть?

Мы понимаем, что в таком анализе существует высокая степень субъективности; тем
не менее, мы использовали два метода, которые обычно применимы. Первый метод -
это хорошо известный подход декомпозиции проблем: по возможности, мы делим
проблемы на отдельные части, которые могут быть решены, объяснены и поняты
относительно независимо. Например, в Raft мы разделили выборы лидеров,
репликацию журналов, безопасность и изменения членства.

Наш второй подход состоял в том, чтобы упростить пространство состояний за счет
сокращения числа рассматриваемых состояний, делая систему более согласованной и
по возможности устраняя недетерминизм. В частности, жерналы не могут иметь дыр,
и Raft ограничивает способы, которыми журналы могут стать несовместимыми друг с
другом. Хотя в большинстве случаев мы пытались устранить недетерминизм, в
некоторых ситуациях недетерминизм действительно улучшает понятность. В
частности, рандомизированные подходы вводят недетерминизм, но они имеют
тенденцию уменьшать пространство состояний, обрабатывая все возможные варианты
подобным образом («выбирайте любой; это не имеет значения»). Мы использовали
рандомизацию, чтобы упростить алгоритм выбора лидера.

## 5. Алгоритм консенсуса Raft {#the-raft-consensus-algorithm}

Raft - это алгоритм управления журналом репликации в форме, описанной в
[разделе 2]({{< relref "#replicated-state-machines" >}}). На рисунке 2
приведен алгоритм в сжатой форме для справки, а на
[рисунке 3]({{< relref "#guarantees">}}) перечислены основные свойства алгоритма; элементы этих рисунков
обсуждаются кусочно в остальной части этого раздела.

Raft реализует консенсус, сначала выбирая лидера, затем предоставляя ему
полную ответственность за управление журналом репликации. Выбранный лиден
принимает записи журнала от клиентов, реплицирует их на другие серверы и сообщает
серверам, когда безопасно применять записи журнала на их конечных автоматах.
Наличие лидера упрощает управление журналом репликации. Например, лидер
может решить, куда поместить новые записи в журнале, не обращаясь к другим
серверам, и данные просто передаются от лидера к другим серверам. Лидер может
выйти из строя или отключиться от других серверов, и в этом случае будет выбран
новый лидер.

Учитывая подход с лидером, Raft разбивает проблему консенсуса на три
относительно независимых подзадачи, которые обсуждаются в следующих подразделах:

- **Выбор лидера**: новый лидер должен быть выбран, когда существующий лидер терпит
  неудачу (Раздел 5.2).

- **Журнал репликации**: лидер должен принимать записи журналов от клиентов
  и реплицировать их по кластеру, заставляя другие журналы соглашаться со своими
  собственными (Раздел 5.3).

- **Безопасность**: ключевым свойством безопасности для Raft является свойство
  безопасности конечного автомата на [рисунке 3]({{< relref "#guarantees" >}}): если какой-либо сервер применил
  определенную запись журнала к своему конечному автомату, то никакой другой
  сервер не может применить другую команду для того же индекса журнала. Раздел
  5.4 описывает, как Raft обеспечивает это свойство; решение включает в себя
  дополнительное ограничение на избирательный механизм, описанный в Разделе 5.2


----
**Безопасность выборов**
: максимум один лидер может быть выбран на каждом сроке.  Раздел 5.2

**Лидер только дописывает**
: лидер никогда не переписывает или удаляет записи своего журнала; он только
дописывает новые записи. Секция 5.3

**Совпадение журналов**
: если два журнала содержат запись с одинаковым индексом и сроком, то эти
журналы совпадают всеми записями до данного индекса. Раздел 5.3

**Полнота лидера**
: если запись в журнале фиксируется в определенный срок, то эта запись будет
присутствовать в журналах лидеров для всех сроков с более высоким номером.
Раздел 5.4

**Безопасность конечного автомата**
: если сервер применил запись журнала с заданным индексом к своему конечному
автомату, ни один другой сервер никогда не применит другую запись журнала для
того же индекса. Раздел 5.4.3
----

{{< figure_wid id="guarantees"
title="Рисунок 3. Гарантии Raft." caption="Raft гарантирует, что каждое из этих свойств является истинным всегда.  Номера разделов указывают, где обсуждается каждое свойство." >}}

После представления алгоритма консенсуса в этом разделе обсуждается вопрос
доступности и роли синхронизации в системе.

### 5.1 Основы Raft {#raft-basics}

Кластер Raft содержит несколько серверов; пять - это типичное число, которое
позволяет системе выдерживать два сбоя. В любой момент времени каждый сервер
находится в одном из трех состояний: лидер (*leader*), последователь (*follower*)
или кандидат (*candidate*). При нормальной работе есть ровно один лидер, а все
остальные серверы являются последователями. Последователи пассивны: они не
делают никаких запросов самостоятельно, а просто отвечают на запросы лидеров и
кандидатов. Лидер обрабатывает все клиентские запросы (если клиент связывается с
подписчиком, он перенаправляет на лидера). Третье состояние, кандидат,
используется для выбора нового лидера, как описано в разделе 5.2. На [рисунке
4]({{< relref "#server_states" >}}) показаны состояния и их переходы; переходы
обсуждаются ниже.

{{< figure_wid id="server_states" src="/img/raft/server_states.png" title="Рисунок 4. Состояния сервера." caption="Последователи отвечают только на запросы с других серверов. Если последователь не получает сообщения, он становится кандидатом и инициирует выборы. Кандидат, который получает голоса от большинства всего кластера, становится новым лидером. Лидеры обычно работают до тех пор, пока не произоёдет сбой." >}}

Raft делит время на сроки (*terms*), как показано на [рисунке 5]({{< relref
"#terms">}}). Сроки нумеруются последовательными целыми числами. Каждый срок
начинается с выборов, на которых один или несколько кандидатов пытаются стать
лидером, как описано в разделе 5.2. Если кандидат побеждает на выборах, он
остается лидером до конца срока. В некоторых ситуациях выборы приводят к
разделению голосов (split vote). В этом случае срок заканчивается без лидера;
новый срок (с новыми выборами) начинается вскоре. Raft гарантирует, что в
каждом сроке есть не более одного лидера.

{{< figure_wid id="terms" src="/img/raft/terms.png" title="Рисунок 5. Сроки." caption="Время делится на сроки, и каждый срок начинается с выборов. После успешных выборов один лидер управляет кластером до конца срока. Некоторые выборы проваливаются, и в этом случае срок заканчивается без выбора лидера. Переходы между сроками могут наблюдаться в разное время на разных серверах." >}} 

Разные серверы могут наблюдать переходы между сроками в разное время, а в
некоторых ситуациях сервер может не наблюдать выборы или даже целые сроки.
Сроки действуют как логические часы [[14]] в Raft и позволяют серверам
обнаруживать неактуальную информацию, такую как устаревшие лидеры. Каждый сервер
хранит текущий номер срока, который монотонно увеличивается со временем. Текущие
сроки обмениваются всякий раз, когда серверы общаются; если текущий срок
одного сервера меньше, чем у другого, он обновляет свой текущий срок до
бо́льшего значения. Если кандидат или лидер обнаруживает, что его срок устарел,
он немедленно возвращается к состоянию последователя. Если сервер получает
запрос с устаревшим номером срока, он отклоняет запрос.

Raft серверы обмениваются данными с использованием удаленных вызовов процедур
(RPC), и для базового алгоритма согласования требуются только два типа RPC.
RequestVote RPC инициируются кандидатами во время выборов (Раздел 5.2), а
AppendEntries RPC инициируются лидерами для репликации записей журнала и
предоставления формы синхронизации (heartbeat) (Раздел 5.3). В разделе 7
добавлен третий RPC для передачи снимков (shapshots) между серверами. Серверы
повторяют RPC запрос, если они не получают ответ своевременно, а для лучшей
производительности они выполняют RPC параллельно.

### 5.2 Выборы лидера {#leader-election}

Raft использует механизм синхронизации (heartbeat), чтобы начать выборы лидера.
Когда серверы стартуют, они находятся в состоянии последователя. Сервер остается
в состоянии последователя, пока он получает валидные RPC запросы от лидера или
кандидата. Лидеры посылают периодические запросы синхронизации (RPC AppendEntries,
которые не содержат записей журнала) всем подписчикам, чтобы сохранить свою
власть (лидерство). Если последователь не получает сообщения в течение периода
времени, называемого таймаутом выборов (*election timeout*), тогда он
предполагает, что нет жизнеспособного лидера, и начинает выборы, чтобы
выбрать нового лидера.

Чтобы начать выборы, последователь увеличивает свой текущий срок и переходит в
состояние кандидата. Затем он голосует за себя и выполняет RPC RequestVote
параллельно каждому из других серверов в кластере. Кандидат остается в этом
состоянии до тех пор, пока не произойдет одна из трех вещей:

- (а) он победит на выборах
- (б) другой сервер утвердит себя в качестве лидера
- (в) пройдет время без победителя

Эти результаты обсуждаются отдельно ниже.

Кандидат выигрывает выборы, если он получает голоса от большинства серверов в
кластере за один и тот же срок. Каждый сервер будет голосовать не более чем за
одного кандидата в определенный срок в порядке поступления заявок (примечание: в
разделе 5.4 добавлено дополнительное ограничение на голоса). Правило большинства
гарантирует, что максимум один кандидат может выиграть выборы на определенный
срок (свойство безопасности выборов на [рисунке 3]({{<relref "#guarantees">}})).
Как только кандидат побеждает на выборах, он становится лидером. Затем он
отправляет сообщения синхронизации (heartbeat) на все остальные серверы,
чтобы установить свои полномочия и предотвратить новые выборы.

В ожидании голосования кандидат может получить RPC AppendEntries от другого
сервера, претендующего на лидерство. Если срок лидера (включенный в RPC),
такой же или больше, чем текущий срок кандидата, тогда кандидат признает лидера
законным и возвращается в состояние последователя. Если срок в RPC меньше, чем
текущий срок кандидата, тогда кандидат отклоняет RPC и переходит в состояние
кандидата.

Третий возможный результат состоит в том, что кандидат не побеждает и не
проигрывает на выборах: если многие последователи становятся кандидатами
одновременно, голоса могут быть разделены так, что ни один из кандидатов не
получит большинства. Когда это происходит, каждый кандидат превысит тайм-аут
и начинёт новые выборы: увеличит свой срок и начнёт новый раунд выборов
через вызов RequestVote RPC. Однако без дополнительных мер такая ситуация
может повторяться бесконечно.

Raft использует случайные сроки оконвания выборов (тайм-ауты), чтобы
гарантировать, что ситуация с равным количеством голосов будут редки
и что они быстро будут разрешены. Во-первых, чтобы предотвратить равное
количество голосов, тайм-ауты выборов выбираются случайным образом
с фиксированным интервалом (например, 150–300 мс). Это распределяет серверы так,
что в большинстве случаев время ожидания истекает только у одного сервера; он
побеждает на выборах и посылает сигналы синхронизации (heartbeat) до истечения
времени ожидания других серверов. Тот же механизм используется для ситуации,
если количество голосов одинаково. Каждый кандидат обновляет свой случайный
тайм-аут в начале выборов, и ожидает истечения этого тайм-аута перед началом
следующих выборов; это уменьшает вероятность ещё одного голосования с одинаковым
числом голосов. Раздел 9.3 показывает, что этот подход быстро выбирает лидера.

Выборы являются примером того, как понятность определяла наш выбор между
вариантами проектирования. Первоначально мы планировали использовать систему
ранжирования: каждому кандидату присваивался уникальный ранг, который
использовался для выбора между конкурирующими кандидатами. Если кандидат
обнаружит другого кандидата с более высоким рангом, он вернется в состояние
последователя, чтобы кандидат с более высоким рейтингом мог легче выиграть
следующие выборы. Мы обнаружили, что при таком подходе возникают тонкие
моменты, связанные с доступностью (серверу с более низким рейтингом может
потребоваться тайм-аут и снова стать кандидатом в случае сбоя сервера с более
высоким рейтингом, но если он делает это слишком рано, он может сбросить
прогресс в выборе лидера). Мы несколько раз вносили коррективы в алгоритм, но
после каждой корректировки появлялись новые частные случаи. В конце концов мы
пришли к выводу, что рандомизированный подход повторения более очевиден и
понятен.

### 5.3 Журнал репликации {#log-replication}


[2]: ...
[4]: ...
[8]: ...
[11]: ...
[13]: ...
[14]: ...
[15]: ...
[16]: ...
[20]: ...
[21]: ...
[22]: ...
[26]: ...
[29]: ...
[32]: ...
[33]: ...
[37]: ...
[38]: ...
[39]: ...
