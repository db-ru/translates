---
title: "В поисках понятного алгоритма консенсуса"
summary: "Raft - это алгоритм консенсуса для управления реплицированным логом.
Он дает результат, эквивалентный (мульти) Paxos, и он так же эффективен, но
его структура отличается от Paxos; это делает Raft более понятным, чем Paxos, а
также обеспечивает лучшую основу для построения практических систем. Чтобы
повысить понятность, Raft разделяет ключевые элементы консенсуса, такие как
выбор лидера, репликация журнала и безопасность, и обеспечивает более высокую
степень согласованности для сокращения числа состояний, которые необходимо
учитывать. Результаты пользовательского исследования показывают, что Raft легче
учиться, чем Paxos. Raft также включает в себя новый механизм изменения членства
в кластере, который использует перекрывающееся большинство для обеспечения
безопасности."
date: 2020-05-16T19:22:56+03:00
draft: true
toc: true
categories:
- paxos
- consencus
- raft
- distributed
---

{{< param Summary >}}

Этот технический отчет является расширенной версией [[32]]; дополнительный
материал отмечен серой полосой на полях. Опубликовано 20 мая 2014 г.

## 0. Оригинал {#origin}

Diego Ongaro and John Ousterhout, [In Search of an Understandable Consensus
Algorithm (Extended Version)](https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf)

## 1. Введение {#introduction}

Консенсусные алгоритмы позволяют совокупности машин работать как единая группа,
которая может пережить сбои (падения) некоторых ее членов. Поэтому они играют
ключевую роль в создании надежных крупномасштабных программных систем. Paxos
[[15], [16]] доминировал в обсуждении алгоритмов консенсуса в течение последнего
десятилетия: большинство реализаций консенсуса основаны на Paxos или под его
влиянием, и Paxos стал основным средством, используемым для обучения студентов
консенсусу.

К сожалению, Paxos довольно сложно понять, даже несмотря на многочисленные
попытки сделать его более доступным. Более того, его архитектура требует сложных
изменений для поддержки практических систем. В результате, как разработчики
систем, так и студенты борются с Paxos.

После того, как мы сами боролись с Paxos, мы решили найти новый алгоритм поиска
консенсуса, который мог бы обеспечить лучшую основу для построения систем и
обучения. Наш подход был необычным в том смысле, что нашей главной целью была
понятность: можем ли мы определить алгоритм консенсуса для практических систем
и описать его так, чтобы его было значительно легче изучить, чем Paxos? Более
того, мы хотели, чтобы алгоритм способствовать развитию интуиции, которые
необходимы для разработчиков систем. Важно было не только, чтобы алгоритм
работал, но и чтобы было понятно, почему он работает.

Результатом этой работы является алгоритм поиска согласованного состояния
(консенсуса) под названием Raft. При разработке Raft мы применяли специальные
методы для улучшения понимания, включая декомпозицию (Raft разделяет выбор
лидера, журнал репликации и безопасность) и сокращение пространства состояний
(по сравнению с Paxos, Raft уменьшает степень недетерминированности и то, как
серверы могут быть несовместимы друг с другом). Пользовательское исследование с
43 студентами в двух университетах показывает, что Raft значительно легче
понять, чем Paxos: после изучения обоих алгоритмов 33 из этих студентов смогли
ответить на вопросы о Raft лучше, чем на вопросы о Paxos.

Raft во многом похож на существующие алгоритмы консенсуса (в частности,
Viewstamped Replication Оки и Лискова [[29], [22]]), но у него есть несколько
нововведений:

- **Сильный лидер**: Raft использует более сильную форму лидерства, чем другие
  алгоритмы консенсуса. Например, записи журнала передаются только от лидера
  к другим серверам. Это упрощает управление журналом репликации и
  облегчает понимание Raft.

- **Выбор лидера**: Raft использует случайные таймеры для выбора лидеров. Это
  добавляет лишь небольшое количество работы к сигналам сихнронизации
  (heartbeat), которые и так требуются для любого алгоритма консенсуса. Но при
  этом упрощаются и ускоряеются разрешения конфликтов.

- **Изменения в членстве**: Механизм Raft для изменения набора серверов в кластере
  использует новый совместный консенсусный подход, при котором большинство двух
  разных конфигураций перекрываются во время переходов. Это позволяет кластеру
  нормально работать во время изменений конфигурации.

Мы считаем, что Raft превосходит Paxos и другие алгоритмы консенсуса, как в
образовательных целях, так и в качестве основы для реализации. Он проще и
понятнее, чем другие алгоритмы; он описан достаточно полно, чтобы удовлетворить
потребности практической системы; он имеет несколько реализаций с открытым
исходным кодом и используется несколькими компаниями; его защитные свойства были
официально определены и доказаны; и его эффективность сопоставима с другими
алгоритмами.

Оставшиеся части статьи знакомят с проблемой реплицированных конечных автоматов
(*replicated state machine, RSM*) ([раздел 2]({{<relref "#replicated-state-machines">}})),
обсуждают сильные и слабые стороны Paxos ([раздел 3]({{<relref "#what-is-wrong-with-paxos">}})),
описывают наш обобщенный подход к пониманиемости (раздел 4),
представляют алгоритм консенсуса Raft (разделы 5–8), дают оценку Raft (Раздел 9)
и обсуждают связанную с этим работу (Раздел 10).

## 2. Реплицированный конечный автомат {#replicated-state-machines}

Алгоритмы консенсуса обычно возникают в контексте реплицированных конечных
автоматов [[37]]. При таком подходе конечные автоматы на нескольких серверах
вычисляют идентичные копии одного и того же состояния и могут продолжать
работать, даже если некоторые из серверов не работают.

Реплицированные конечные автоматы используются для решения различных проблем
отказоустойчивости в распределенных системах. Например, в больших масштабируемых
системах, в которых один лидер в кластере, таких как GFS [[8]], HDFS [[38]] и
RAMCloud [[33]], обычно используется отдельный реплицированный конечный автомат
для управления выбором лидера и хранения информации о конфигурации, которая
должна сохраниться при падении лидера. Примеры реплицированных конечных
автоматов включают Chubby [[2]] и ZooKeeper [[11]].

Реплицированные конечные автоматы обычно реализуются с использованием
журнала репликации, как показано на [рисунке 1]({{< relref "#rsm_arch" >}}). Каждый сервер хранит
журнал, содержащий серию команд, которые его конечный автомат выполняет по
порядку. Каждый журнал содержит одинаковые команды в одинаковом порядке, поэтому
каждый конечный автомат обрабатывает одну и ту же последовательность команд.
Поскольку конечные автоматы являются детерминированными, каждый вычисляет одно и
то же состояние и одну и ту же последовательность выходных данных.

{{< figure_wid id="rsm_arch" src="/img/raft/rsm_arch.png" title="Рисунок 1. Архитектура реплицированного конечного автомата." caption="Алгоритм консенсуса (consensus module) управляет журналом репликации (Log), содержащим команды конечного автомата от клиентов (clients). Конечные автоматы обрабатывают идентичные последовательности команд из журналов, поэтому они выдают одинаковые выходные данные." >}}

Поддержка лога репликации в согласованном состоянии - задача алгоритма
консенсуса. Модуль консенсуса на сервере получает команды от клиентов и
добавляет их в свой журнал. Он общается с модулями консенсуса на других
серверах, чтобы гарантировать, что каждый журнал в конечном итоге содержит
одинаковые запросы в одинаковом порядке, даже если некоторые серверы выходят из
строя. Как только команды отреплицированы, конечный автомат каждого сервера
обрабатывает их в порядке ведения журнала, а выходные данные возвращаются
клиентам. В результате серверы образуют единый высоконадежный конечный автомат.

Алгоритмы консенсуса на практике обычно имеют следующие свойства:

- Они гарантируют **безопасность** (никогда не возвращая неверный результат) при
  всех невизантийских условиях, включая сетевые задержки, разбиения и потери
  пакетов, дублирование и переупорядочение.

- Они полностью функциональны (**доступны**) пока большинство серверов
  работает и может связываться друг с другом и с клиентами. Таким образом,
  типичный кластер из пяти серверов может терпеть сбой любых двух серверов.
  Предполагается, что серверы выходят из строя, если они остановлены; они
  могут позже восстановиться из состояния на стабильном хранилище и вернуться
  в кластер.

- Они не зависят от времени для обеспечения согласованности журналов:
  неисправные часы и экстремальные задержки сообщений могут в худшем случае
  вызвать проблемы с доступностью.

- В общем случае команда может завершиться, как только бо́льшая часть кластера
  ответит на один раунд удаленных вызовов процедур; меньшинство медленных
  серверов не должно влиять на общую производительность системы.

## 3. Что не так с Paxos? {#what-is-wrong-with-paxos}

За последние десять лет протокол Paxos Лесли Лэмпорта [15] стал почти синонимом
консенсуса: этот протокол чаще всего преподается на курсах, и большинство
реализаций консенсуса используют его в качестве отправной точки. Сначала Paxos
определяет протокол, способный достичь согласия (консенсуса) по одному решению,
например, по записи в журнале репликации. Мы называем это подмножество как
одно-указный Paxos (*single-decree Paxos*). Затем Paxos объединяет несколько
экземпляров этого протокола для облегчения серии решений, таких как журнал
(*multi-Paxos*). Paxos обеспечивает безопасность (safety) и жизнеспособность
(liveness), а также поддерживает изменения в составе кластера. Его правильность
доказана, и в обычном случае он эффективен.

К сожалению, у Paxos есть два существенных недостатка. Первый недостаток в том,
что Paxos исключительно сложен для понимания. Всем известно, что полное
объяснение [[15]] непрозрачно; немногие люди понимают его, и только с большими
усилиями. В результате было несколько попыток объяснить Paxos в более простых
терминах [[16], [20], [21]]. Эти объяснения сосредоточены на подмножестве
одно-указного Paxos (single-decree Paxos), но и они всё ещё остаются сложными.
В неофициальном опросе участников на NSDI 2012, мы обнаружили мало людей,
которым было бы удобно с Paxos, даже среди опытных исследователей. Мы сами
боролись с Paxos; мы не могли понять протокол полностью до тех пор, пока не
прочитали несколько упрощенных объяснений и не разработали собственный
альтернативный протокол - процесс, который занял почти год.

Мы предполагаем, что непрозрачность Paxos исходит из выбора в качестве основы
одно-указного подмножества (single-decree). Одно-указный Paxos непрозрачный и
хрупкий: он разделен на две стадии, которые не имеют простых наглядных
объяснений и не могут быть поняты независимо. Из-за этого трудно интуитивно
понять, почему этот протокол работает. Правила композиции multi-Paxos
добавляют значительную дополнительную сложность и хрупкость. Мы считаем, что
общая проблема достижения консенсуса по нескольким решениям (таким как: журнал
вместо одной записи) может быть декомпозирована другими способами, которые
являются более точными и очевидными.

Вторая проблема Paxos заключается в том, что он не обеспечивает хорошую основу
для создания практических реализаций. Одна из причин заключается в том, что не
существует широко согласованного алгоритма для multi-Paxos. Описания Лампорта в
основном касаются single-decree Paxos; он набросал возможные подходы к
multi-Paxos, но многие детали отсутствуют. Было несколько попыток
конкретизировать и оптимизировать Paxos, такие как [[26]], [[39]] и [[13]],
но они отличаются друг от друга и от эскизов Лампорта. В таких системах,
как Chubby [[4]], реализованы алгоритмы, подобные Paxos, но в большинстве случаев их
подробности не публикуются.

Помимо этого, архитектура Paxos плоха для построения практических систем; это
еще одно следствие single-decree декомпозиции. Например, нет особого
преимущества, если вы выбираете набор записей журнала независимо, а затем
объединяете их в последовательный журнал; это только добавляет сложности. 
Проще и эффективнее спроектировать систему вокруг журнала, в которой новые
записи добавляются последовательно в ограниченном порядке. Другая проблема
заключается в том, что Paxos использует симметричный одноранговый (peer-to-peer)
подход в своей основе (хотя в конечном итоге предлагается слабая форма
лидерства в качестве оптимизации производительности). Это имеет смысл в
упрощенном мире, где будет принято только одно решение, и лишь немногие
системы на практике используют этот подход. Если необходимо принять ряд решений,
проще и быстрее сначала выбрать лидера, а затем назначить лидера для координации
решений.

В результате, практические системы мало похожи на Paxos. Каждая реализация
начинается с Paxos, обнаруживает трудности в ее реализации, а затем
разрабатывает существенно другую архитектуру. Это отнимает много времени и
подвержено ошибкам, а трудности понимания Paxos усугубляют проблему.
Формулировка Paxos может быть хорошей для доказательства теорем о ее
правильности, но реальные реализации настолько отличаются от Paxos, что
доказательства имеют небольшую ценность. Следующий комментарий от разработчиков
Chubby является типичным:

> Существуют значительные пробелы между описанием алгоритма Paxos и
> потребностями реальной системы... окончательная система будет основана на
> недоказанном протоколе [[4]].

Из-за этих проблем мы пришли к выводу, что Paxos не обеспечивает хорошей основы
ни для разработки систем, ни для образования. Учитывая важность консенсуса в
больших масштабируемых системах, мы решили посмотреть, сможем ли мы
разработать альтернативный алгоритм консенсуса с лучшими свойствами, чем у
Paxos.

## 4. Проектирование для понятности {#designing-for-understandability}

У нас было несколько целей при разработке Raft: он должен обеспечить полную и
практическую основу для разработки систем, чтобы значительно сократить объем
проектных работ, требуемых от разработчиков; он должен быть безопасным при любых
условиях и доступным при обычных условиях эксплуатации; и он должен быть
эффективным для обычных операций. Но нашей самой важной целью — и самой сложной
задачей - была *понятность*. Важно, чтобы алгоритмы был понятен для большой
аудитории без особых сложностей. Дополнительно, алгоритм должен быть
интуитивным, чтобы разработчики могли создавать расширения, которые неизбежны
в реальных реализациях.

В дизайне Raft было много моментов, когда нам приходилось выбирать среди
альтернативных подходов. В этих ситуациях мы оценивали альтернативы на основе
понятности: насколько сложно объяснить каждую альтернативу (например, насколько
сложным является её пространство состояний и имеет ли она неочевидные
моменты?) И насколько легко читателю будет полностью понять подход и его
суть?

Мы понимаем, что в таком анализе существует высокая степень субъективности; тем
не менее, мы использовали два метода, которые обычно применимы. Первый метод -
это хорошо известный подход декомпозиции проблем: по возможности, мы делим
проблемы на отдельные части, которые могут быть решены, объяснены и поняты
относительно независимо. Например, в Raft мы разделили выборы лидеров,
репликацию журналов, безопасность и изменения членства.

Наш второй подход состоял в том, чтобы упростить пространство состояний за счет
сокращения числа рассматриваемых состояний, делая систему более согласованной и
по возможности устраняя недетерминизм. В частности, жерналы не могут иметь дыр,
и Raft ограничивает способы, которыми журналы могут стать несовместимыми друг с
другом. Хотя в большинстве случаев мы пытались устранить недетерминизм, в
некоторых ситуациях недетерминизм действительно улучшает понятность. В
частности, рандомизированные подходы вводят недетерминизм, но они имеют
тенденцию уменьшать пространство состояний, обрабатывая все возможные варианты
подобным образом («выбирайте любой; это не имеет значения»). Мы использовали
рандомизацию, чтобы упростить алгоритм выбора лидера.

## 5. Алгоритм консенсуса Raft {#the-raft-consensus-algorithm}

Raft - это алгоритм управления журналом репликации в форме, описанной в
[разделе 2]({{< relref "#replicated-state-machines" >}}). На рисунке 2
приведен алгоритм в сжатой форме для справки, а на
[рисунке 3]({{< relref "#guarantees">}}) перечислены основные свойства алгоритма; элементы этих рисунков
обсуждаются кусочно в остальной части этого раздела.

Raft реализует консенсус, сначала выбирая лидера, затем предоставляя ему
полную ответственность за управление журналом репликации. Выбранный лиден
принимает записи журнала от клиентов, реплицирует их на другие серверы и сообщает
серверам, когда безопасно применять записи журнала на их конечных автоматах.
Наличие лидера упрощает управление журналом репликации. Например, лидер
может решить, куда поместить новые записи в журнале, не обращаясь к другим
серверам, и данные просто передаются от лидера к другим серверам. Лидер может
выйти из строя или отключиться от других серверов, и в этом случае будет выбран
новый лидер.

Учитывая подход с лидером, Raft разбивает проблему консенсуса на три
относительно независимых подзадачи, которые обсуждаются в следующих подразделах:

- **Выбор лидера**: новый лидер должен быть выбран, когда существующий лидер терпит
  неудачу (Раздел 5.2).

- **Журнал репликации**: лидер должен принимать записи журналов от клиентов
  и реплицировать их по кластеру, заставляя другие журналы соглашаться со своими
  собственными (Раздел 5.3).

- **Безопасность**: ключевым свойством безопасности для Raft является свойство
  безопасности конечного автомата на [рисунке 3]({{< relref "#guarantees" >}}): если какой-либо сервер применил
  определенную запись журнала к своему конечному автомату, то никакой другой
  сервер не может применить другую команду для того же индекса журнала. Раздел
  5.4 описывает, как Raft обеспечивает это свойство; решение включает в себя
  дополнительное ограничение на избирательный механизм, описанный в Разделе 5.2


----
**Безопасность выборов** (Election Safety)
: максимум один лидер может быть выбран на каждом сроке.  Раздел 5.2

**Лидер только дописывает** (Leader Append-Only)
: лидер никогда не переписывает или удаляет записи своего журнала; он только
дописывает новые записи. Секция 5.3

**Совпадение журналов** (Log Matching)
: если два журнала содержат запись с одинаковым индексом и сроком, то эти
журналы совпадают всеми записями до данного индекса. Раздел 5.3

**Полнота лидера** (Leader Completeness)
: если запись в журнале фиксируется в определенный срок, то эта запись будет
присутствовать в журналах лидеров для всех сроков с более высоким номером.
Раздел 5.4

**Безопасность конечного автомата** (State Machine Safety)
: если сервер применил запись журнала с заданным индексом к своему конечному
автомату, ни один другой сервер никогда не применит другую запись журнала для
того же индекса. Раздел 5.4.3
----

{{< figure_wid id="guarantees"
title="Рисунок 3. Гарантии Raft." caption="Raft гарантирует, что каждое из этих свойств является истинным всегда.  Номера разделов указывают, где обсуждается каждое свойство." >}}

После представления алгоритма консенсуса в этом разделе обсуждается вопрос
доступности и роли синхронизации в системе.

### 5.1 Основы Raft {#raft-basics}

Кластер Raft содержит несколько серверов; пять - это типичное число, которое
позволяет системе выдерживать два сбоя. В любой момент времени каждый сервер
находится в одном из трех состояний: лидер (*leader*), последователь (*follower*)
или кандидат (*candidate*). При нормальной работе есть ровно один лидер, а все
остальные серверы являются последователями. Последователи пассивны: они не
делают никаких запросов самостоятельно, а просто отвечают на запросы лидеров и
кандидатов. Лидер обрабатывает все клиентские запросы (если клиент связывается с
последователем, он перенаправляется на лидера). Третье состояние, кандидат,
используется для выбора нового лидера, как описано в разделе 5.2. На [рисунке
4]({{< relref "#server_states" >}}) показаны состояния и их переходы; переходы
обсуждаются ниже.

{{< figure_wid id="server_states" src="/img/raft/server_states.png" title="Рисунок 4. Состояния сервера." caption="Последователи отвечают только на запросы с других серверов. Если последователь не получает сообщения, он становится кандидатом и инициирует выборы. Кандидат, который получает голоса от большинства всего кластера, становится новым лидером. Лидеры обычно работают до тех пор, пока не произоёдет сбой." >}}

Raft делит время на сроки (*terms*), как показано на [рисунке 5]({{< relref
"#terms">}}). Сроки нумеруются последовательными целыми числами. Каждый срок
начинается с выборов, на которых один или несколько кандидатов пытаются стать
лидером, как описано в разделе 5.2. Если кандидат побеждает на выборах, он
остается лидером до конца срока. В некоторых ситуациях выборы приводят к
разделению голосов (split vote). В этом случае срок заканчивается без лидера;
новый срок (с новыми выборами) начинается вскоре. Raft гарантирует, что в
каждом сроке есть не более одного лидера.

{{< figure_wid id="terms" src="/img/raft/terms.png" title="Рисунок 5. Сроки." caption="Время делится на сроки, и каждый срок начинается с выборов. После успешных выборов один лидер управляет кластером до конца срока. Некоторые выборы проваливаются, и в этом случае срок заканчивается без выбора лидера. Переходы между сроками могут наблюдаться в разное время на разных серверах." >}} 

Разные серверы могут наблюдать переходы между сроками в разное время, а в
некоторых ситуациях сервер может не наблюдать выборы или даже целые сроки.
Сроки действуют как логические часы [[14]] в Raft и позволяют серверам
обнаруживать неактуальную информацию, такую как устаревшие лидеры. Каждый сервер
хранит текущий номер срока, который монотонно увеличивается со временем. Текущие
сроки обмениваются всякий раз, когда серверы общаются; если текущий срок
одного сервера меньше, чем у другого, он обновляет свой текущий срок до
бо́льшего значения. Если кандидат или лидер обнаруживает, что его срок устарел,
он немедленно возвращается к состоянию последователя. Если сервер получает
запрос с устаревшим номером срока, он отклоняет запрос.

Raft серверы обмениваются данными с использованием удаленных вызовов процедур
(RPC), и для базового алгоритма согласования требуются только два типа RPC.
`RequestVote` RPC инициируются кандидатами во время выборов (Раздел 5.2), а
`AppendEntries` RPC инициируются лидерами для репликации записей журнала и
предоставления формы синхронизации (heartbeat) (Раздел 5.3). В разделе 7
добавлен третий RPC для передачи снимков (shapshots) между серверами. Серверы
повторяют RPC запрос, если они не получают ответ своевременно, а для лучшей
производительности они выполняют RPC параллельно.

### 5.2 Выборы лидера {#leader-election}

Raft использует механизм синхронизации (heartbeat), чтобы начать выборы лидера.
Когда серверы стартуют, они находятся в состоянии последователя. Сервер остается
в состоянии последователя, пока он получает валидные RPC запросы от лидера или
кандидата. Лидеры посылают периодические запросы синхронизации (RPC `AppendEntries`,
которые не содержат записей журнала) всем последователям, чтобы сохранить свою
власть (лидерство). Если последователь не получает сообщения в течение периода
времени, называемого таймаутом выборов (*election timeout*), тогда он
предполагает, что нет жизнеспособного лидера, и начинает выборы, чтобы
выбрать нового лидера.

Чтобы начать выборы, последователь увеличивает свой текущий срок и переходит в
состояние кандидата. Затем он голосует за себя и выполняет RPC `RequestVote`
параллельно каждому из других серверов в кластере. Кандидат остается в этом
состоянии до тех пор, пока не произойдет одна из трех вещей:

- (а) он победит на выборах
- (б) другой сервер утвердит себя в качестве лидера
- (в) пройдет время без победителя

Эти результаты обсуждаются отдельно ниже.

Кандидат выигрывает выборы, если он получает голоса от большинства серверов в
кластере за один и тот же срок. Каждый сервер будет голосовать не более чем за
одного кандидата в определенный срок в порядке поступления заявок (примечание: в
разделе 5.4 добавлено дополнительное ограничение на голоса). Правило большинства
гарантирует, что максимум один кандидат может выиграть выборы на определенный
срок (свойство безопасности выборов на [рисунке 3]({{<relref "#guarantees">}})).
Как только кандидат побеждает на выборах, он становится лидером. Затем он
отправляет сообщения синхронизации (heartbeat) на все остальные серверы,
чтобы установить свои полномочия и предотвратить новые выборы.

В ожидании голосования кандидат может получить RPC `AppendEntries` от другого
сервера, претендующего на лидерство. Если срок лидера (включенный в RPC),
такой же или больше, чем текущий срок кандидата, тогда кандидат признает лидера
законным и возвращается в состояние последователя. Если срок в RPC меньше, чем
текущий срок кандидата, тогда кандидат отклоняет RPC и переходит в состояние
кандидата.

Третий возможный результат состоит в том, что кандидат не побеждает и не
проигрывает на выборах: если многие последователи становятся кандидатами
одновременно, голоса могут быть разделены так, что ни один из кандидатов не
получит большинства. Когда это происходит, каждый кандидат превысит тайм-аут
и начинёт новые выборы: увеличит свой срок и начнёт новый раунд выборов
через вызов `RequestVote` RPC. Однако без дополнительных мер такая ситуация
может повторяться бесконечно.

Raft использует случайные сроки оконвания выборов (тайм-ауты), чтобы
гарантировать, что ситуация с равным количеством голосов будут редки
и что они быстро будут разрешены. Во-первых, чтобы предотвратить равное
количество голосов, тайм-ауты выборов выбираются случайным образом
с фиксированным интервалом (например, 150–300 мс). Это распределяет серверы так,
что в большинстве случаев время ожидания истекает только у одного сервера; он
побеждает на выборах и посылает сигналы синхронизации (heartbeat) до истечения
времени ожидания других серверов. Тот же механизм используется для ситуации,
если количество голосов одинаково. Каждый кандидат обновляет свой случайный
тайм-аут в начале выборов, и ожидает истечения этого тайм-аута перед началом
следующих выборов; это уменьшает вероятность ещё одного голосования с одинаковым
числом голосов. Раздел 9.3 показывает, что этот подход быстро выбирает лидера.

Выборы являются примером того, как понятность определяла наш выбор между
вариантами проектирования. Первоначально мы планировали использовать систему
ранжирования: каждому кандидату присваивался уникальный ранг, который
использовался для выбора между конкурирующими кандидатами. Если кандидат
обнаружит другого кандидата с более высоким рангом, он вернется в состояние
последователя, чтобы кандидат с более высоким рейтингом мог легче выиграть
следующие выборы. Мы обнаружили, что при таком подходе возникают тонкие
моменты, связанные с доступностью (серверу с более низким рейтингом может
потребоваться тайм-аут и снова стать кандидатом в случае сбоя сервера с более
высоким рейтингом, но если он делает это слишком рано, он может сбросить
прогресс в выборе лидера). Мы несколько раз вносили коррективы в алгоритм, но
после каждой корректировки появлялись новые частные случаи. В конце концов мы
пришли к выводу, что рандомизированный подход повторения более очевиден и
понятен.

### 5.3 Журнал репликации {#log-replication}

Как только лидер избран, он начинает обслуживать запросы клиентов. Каждый
клиентский запрос содержит команду, которая должна быть выполнена
реплицированными конечными автоматами. Лидер добавляет эту команду в свой
журнал в качестве новой записи, а затем параллельно вызывает RPC `AppendEntries`
для каждого из серверов для репликации записи. Когда запись была благополучно
реплицирована (как описано ниже), лидер применяет запись к своему конечному
автомату и возвращает результат этого выполнения клиенту. Если последователи
аварийно завершают работу или работают медленно, или если сетевые пакеты
потеряны, лидер бесконечно повторяет RPC вызов `AppendEntries` (даже после того,
как он ответил клиенту), пока все последователи в конечном итоге не сохранят все
записи журнала.

Журналы организованы, как показано на [рисунке 6]({{< relref "#logs" >}}). Каждая
запись журнала хранит команду конечного автомата вместе с номером срока, когда
запись была получена лидером. Номера сроков в записях журнала используются
для обнаружения несоответствий между журналами и для обеспечения некоторых
свойств на [рисунке 3]({{< relref "#guarantees">}}).
Каждая запись журнала также имеет целочисленный индекс, определяющий ее
положение в журнале.

{{< figure_wid id="logs" src="/img/raft/logs.png"
title="Рисунок 6. Журнал"
caption="Журналы состоят из записей, которые нумеруются последовательно. Каждая запись содержит срок, в котором она была создана (номер в каждом квадрате) и команду для конечного автомата. Запись считается зафиксированной (*committed*), если её можно безопасно примененить к конечному автоматаму." >}} 

Лидер решает, когда безопасно применить запись журнала к конечным автоматам;
такая запись называется зафиксированной (*committed*). Raft гарантирует, что
зафиксированные записи долговечны и в конечном итоге будут выполняться всеми
доступными конечными автоматами (на всех серверах). Запись в журнале
фиксируется после того, как лидер, создавший эту запись, реплицировал её на
большинство серверов (например, запись 7 на [рисунке 6]({{< relref "#logs" >}})). Это также фиксирует все
предыдущие записи в журнале лидера, включая записи, созданные предыдущими
лидерами. В разделе 5.4 обсуждаются некоторые тонкости при применении этого
правила после смены лидера, а также показано, что это определение фиксации
является безопасным. Лидер отслеживает самый большой индекс, который он знает,
чтобы быть зафиксированным, и он включает этот индекс в будущие RPC вызовы
`AppendEntries` (включая импульсы синхронизации), чтобы другие серверы в конечном
итоге так же получили записи. Как только последователь узнает, что запись в
журнале зафиксирована, он применяет эту запись к своему локальному конечному
автомату (в порядке следования записей в журнале).

Мы спроектировали механизм журналов Raft так, чтобы обеспечить высокий уровень
согласованности между журналами на разных серверах. Это не только упрощает
поведение системы и делает её более предсказуемой, но и является важным
компонентом обеспечения безопасности. Raft поддерживает следующие свойства,
которые вместе составляют свойство *Совпадения Журналов* (Log Matching Property)
на [рисунке 3]({{< relref "#guarantees" >}}):

- Если две записи в разных журналах имеют одинаковый индекс и срок, то они
  хранят одну и ту же команду.
- Если две записи в разных журналах имеют одинаковый индекс и срок, тогда
  журналы идентичны во всех предыдущих записях.

Первое свойство вытекает из того факта, что лидер создает не более одной записи
с заданным индексом журнала за данный срок, и записи журнала никогда не меняют
свою позицию в журнале. Второе свойство гарантируется простой проверкой
согласованности, выполняемой `AppendEntries`. При отправке RPC `AppendEntries`
лидер включает в свой журнал индекс и срок записи, которая непосредственно
предшествует новым записям. Если последователь не находит в своем журнале запись с
таким же индексом и сроком, то он отклоняет новые записи. Проверка
согласованности действует как вводный этап: начальное пустое состояние журналов
удовлетворяет свойству *соответствия журналов*, а проверка согласованности
сохраняет свойство *соответствия журналов* при каждом расширении журналов. В
результате, всякий раз, когда `AppendEntries` возвращается успешно, лидер знает,
что журнал последователя идентичен его собственному журналу вплоть до новых
записей.

Во время нормальной работы журналы лидера и последователей остаются согласованными,
поэтому проверка согласованности `AppendEntries` никогда не завершается ошибкой.
Однако из-за сбоев лидера журналы могут быть непоследовательными (старый лидер,
возможно, не полностью реплицировал все записи в своем журнале). Эти
несоответствия могут усугубляться рядом сбоев лидера и последователя. На
[рисунке 7]({{< relref "#logs2">}}) показаны отличия журналов последователей
от записей нового лидера. У последователя могут отсутствовать записи,
присутствующие в лидере, у него могут быть дополнительные записи, отсутствующие
у лидера, или и то, и другое. Отсутствующие и посторонние записи в журнале
могут охватывать несколько сроков.

{{< figure_wid id="logs2" src="/img/raft/logs.2.png"
title="Рисунок 7. Отличия журналов"
caption="Когда лидер наверху приходит к власти, возможно, что любой из сценариев (a-f) может произойти в журналах последователей. Каждый квадрат представляет одну запись журнала; число в квадрате - это его срок. У последователя могут отсутствовать записи (a – b), могут быть дополнительные незафиксированные записи (c – d) или и то и другое (e – f). Например, сценарий (f) мог произойти, если этот сервер являлся лидером для срока 2, добавил несколько записей в свой журнал, а затем «упал» перед фиксацией своих записей; он быстро перезапустился, стал лидером на срок 3 и добавил еще несколько записей в свой журнал; до того, как какие-либо записи в сроках 2 или 3 были зафиксированы, сервер снова вышел из строя и оставался недоступным в течении нескольких сроков" >}} 

В Raft лидер обрабатывает несоответствия, заставляя журналы последователей
дублировать свои собственные. Это означает, что конфликтующие записи в журналах
последователей будут перезаписаны записями из журнала лидера. Раздел 5.4 покажет,
что это безопасно в сочетании с еще одним ограничением.

Чтобы привести журнал последователя в соответствие с собственным, лидер
должен найти последнюю запись в журнале, где оба журнала согласуются, удалить
все записи в журнале последователя после этой точки и отправить последователю все
записи лидера после этой точки.  Все эти действия происходят в ответ на проверку
согласованности, выполняемую RPC `AppendEntries`. Лидер поддерживает *nextIndex*
для каждого последователя, который является индексом следующей записи журнала,
которую лидер отправит этому последователю. Когда лидер приходит к власти, он
первым делом инициализирует все значения `nextIndex` последним значением
индекса из своего журнала, увеличенным на единицу (индекс 11 на рисунке 7).
Если журнал последователя не соответствует журналу лидера, проверка
согласованности `AppendEntries` завершится неудачно при следующем вызове
RPC `AppendEntries`. После отказа лидер уменьшит значение `nextIndex` и
повторит RPC `AppendEntries`. В конечном счете `nextIndex` достигнет точки,
где совпадают журналы лидера и последователя. Когда это произойдет,
`AppendEntries` будет успешным, в результате чего устранятся любые
конфликтующие записи в журнале последователя и добавятся записи из журнала
лидера (если таковые имеются). После успешного выполнения `AppendEntries`
журнал последователя будет согласованным с журналом лидера, и он останется
таким до конца срока.

> При желании протокол можно оптимизировать, чтобы уменьшить количество
> отклоненных RPC `AppendEntries`. Например, когда отклоняется запрос
> `AppendEntries` последователь может включать сорок конфликтующей записи и
> первый индекс, который он сохраняет для этого срока. Имея эту информацию, лидер
> может уменьшить значение `nextIndex`, чтобы пропустить все конфликтующие записи
> в этом сроке. Таким образом, потребуется только один RPC `AppendEntries`
> для каждого срока, где есть конфликтующие записи, а не по одному RPC для
> каждой записи. На практике мы сомневаемся, что эта оптимизация необходима,
> поскольку сбои случаются нечасто и маловероятно, что будет много несогласованных
> записей.

С помощью этого механизма лидеру не нужно предпринимать каких-либо
специальных действий для восстановления согласованности журналов, когда он
приходит к власти. Он просто начинает обычную работу, и журналы автоматически
сходятся в ответ на сбои проверки согласованности в `AppendEntries`. Лидер никогда
не перезаписывает и не удаляет записи в своем собственном журнале (свойство
*Лидер только дописывает* (Leader Append-Only) на [рисунке 3]({{< relref "#guarantees">}})).

Этот механизм репликации журнала демонстрирует требуемые свойства консенсуса,
описанные в [разделе 2]({{< relref "#replicated-state-machines">}}): Raft
может принимать, реплицировать и применять новые записи журнала, пока работает
большинство серверов; в обычном случае новая запись может быть реплицирована за
один цикл вызовов RPC для большинства кластера; и один медленный последователь
не повлияет на производительность.


### 5.4 Безопасность {#safety}

В предыдущих разделах описано, как Raft выбирает лидеров и реплицирует записи в
журнале. Однако описанных механизмов недостаточно для обеспечения того, чтобы
каждый конечный автомат выполнял одинаковые команды в одинаковом порядке.
Например, последователь может быть недоступен, пока лидер фиксирует несколько
записей журнала, потом он может быть избран лидером и перезаписать эти записи
новыми; в результате разные конечные автоматы могут выполнять разные
последовательности команд.

Этот раздел завершает описание алгоритма Raft, добавляя ограничение на серверы,
которые могут быть выбраны лидерами. Ограничение гарантирует, что лидер для
любого данного срока содержит все записи, зафиксированные в предыдущих
сроках (свойство *Полноты лидера* (Leader Completeness) из [рисунка 3]({{<
relref "#guarantees">}})). Учитывая ограничение выборов лидера, мы затем
уточняем правила фиксации. Наконец, мы представляем пробный эскиз для свойства
полноты лидера (Leader Completeness) и показываем, как это приводит к
правильному поведению реплицированного конечного автомата.

#### 5.4.1 Ограничение выборов {#election-restriction}

В любом алгоритме косенсуса, который реализован на базе лидера, лидер должен в
конечном итоге сохранить все зафиксированные записи журнала. В некоторых
алгоритмах консенсуса, таких как Viewstamped Replication [[22]], лидер может
быть выбран, даже если он изначально не содержит всех зафиксированных записей.
Эти алгоритмы содержат дополнительные механизмы для выявления пропущенных
записей и передачи их новому лидеру либо во время процесса выборов, либо вскоре
после этого. К сожалению, это приводит к значительному усложнению. Raft
использует более простой подход: он гарантирует, что все зафиксированные
записи из предыдущих сроков присутствуют у каждого нового лидера с момента его
избрания, без необходимости передавать эти записи лидеру. Это означает, что
записи журнала передаются только в одном направлении, от лидеров к
последователям, и лидеры никогда не перезаписывают существующие записи в
своих журналах.

Raft использует процесс голосования, чтобы помешать кандидату выиграть выборы,
если его журнал не содержит всех зафиксированных записей. Кандидат должен
связаться с большинством кластера, чтобы быть избранным, что означает, что
каждая зафиксированная запись должна присутствовать хотя бы на одном из этих
серверов. Если журнал кандидата по крайней мере так же актуален, как и любой
другой журнал в этом большинстве (точное значение «актуален» определено ниже),
то в нем будут храниться все зафиксированные записи. RPC `RequestVote` реализует
это ограничение: RPC вызов включает информацию о журнале кандидата, и
избиратель отказывает в голосовании, если его собственный журнал более
актуален, чем у кандидата.

Raft определяет, какой из двух журналов более «актуален», сравнивая индекс и срок
действия последних записей в журналах. Если в журналах есть последние записи с
разными сроками, то журнал с более поздним сроком является более актуальным.
Если журналы заканчиваются одним и тем же сроком, то актуальным явлется более
длинный журнал.

#### 5.4.2 Фиксация записей из предыдущих сроков {#committing-entries-from-previous-terms}

Как описано в [Разделе 5.3]({{< relref "#log-replication">}}), лидер знает, что запись из его текущего срока
фиксируется только тогда, когда эта запись сохранится на большинстве серверов.
Если лидер «падает» до фиксации записи, будущие лидеры попытаются завершить
репликацию записи. Однако лидер не может сразу понять, что запись
предыдущего срока зафиксирована, так как она хранится на большинстве серверов.
На [рисунке 8]({{< relref "#fig8">}}) показана ситуация, когда старая запись журнала хранится на
большинстве серверов, но все же может быть перезаписана будущим лидером.

{{< figure_wid id="fig8" src="/img/raft/fig.8.png"
title="Рисунок 8."
caption="Временная последовательность, показывающая, почему лидер не может определить фиксацию, используя записи журнала из более старых сроков.  В (a) S1 является лидером и частично копирует запись журнала в индексе 2.  В (b) S1 падает; S5 избирается лидером на срок 3 с голосами от S3, S4 и самой себя, и принимает другую запись в журнале индекса 2.  В (c) S5 падает; S1 перезапускается, выбирается лидером и продолжает репликацию.  На этом этапе запись журнала из 2ого срока была реплицирована на большинство серверов, но не зафиксирована. Если S1 падает, как в (d), S5 может быть выбран лидером (с голосами от S2, S3 и S4) и перезаписать запись своей собственной записью из 3го срока. Однако, если S1 реплицирует запись из своего текущего срока на большинство серверов перед сбоем, как в (е), то затем эта запись фиксируется (S5 не может выиграть выборы). На этом этапе все предыдущие записи в журнале также фиксируются." >}} 

Чтобы устранить проблемы, подобные той, что показана на рисунке 8, Raft никогда
не фиксирует записи в журналах из предыдущих терминов путем подсчета реплик.
Только записи журнала с текущего срока лидера фиксируются путем подсчета реплик;
как только запись из текущего срока была зафиксирована таким образом, то все
предыдущие записи фиксируются косвенно из-за свойства совпадения журнала (Log
Matching Property). В некоторых ситуациях лидер может с уверенностью заключить,
что более старая запись в журнале зафиксирована (например, если эта запись
хранится на каждом сервере), но Raft использует более консервативный подход для
простоты.

Raft включает эту дополнительную сложность в правилах фиксации,
поскольку записи журнала сохраняют свои исходные номера сроков, когда лидер
реплицирует записи из предыдущих сроков. В других алгоритмах консенсуса, если
новый лидер реплицирует записи из предыдущих сроков, он должен делать это со
своим новым номером срока. Подход Raft облегчает рассуждения о записях в
журнале, поскольку они поддерживают один и тот же номер срока во времени и в
разных журналах. Кроме того, новые лидеры в Raft отправляют меньше записей
журнала из предыдущих сроков, чем в других алгоритмах (другие алгоритмы должны
отправлять избыточные записи журнала, чтобы перенумеровать их, прежде чем они
будут зафиксированы).


[2]: ...
[4]: ...
[8]: ...
[11]: ...
[13]: ...
[14]: ...
[15]: ...
[16]: ...
[20]: ...
[21]: ...
[22]: ...
[26]: ...
[29]: ...
[32]: ...
[33]: ...
[37]: ...
[38]: ...
[39]: ...
