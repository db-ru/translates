---
title: "Readings in Database Systems 5. Масштабируемые механизмы обработки данных"
date: 2020-08-23T12:52:58+03:00
summary: "Из многих разработок в области управления данными за последнее
десятилетие MapReduce и последующие крупномасштабные системы обработки данных
стали одними из самых пробивных и самых противоречивых. Низкая стоимость
хранения данных и их растущие объемы побудили многих поставщиков интернет-услуг
отказаться от традиционных баз данных и хранилищ, а вместо этого создать
свои собственные механизмы."
categories:
- readings in database systems
- red book
- map-reduce
- google file system
- dataflow engine
- Peter Bailis
draft: true
---

> Избранные статьи:
> 
> - Jeff Dean and Sanjay Ghemawat. MapReduce: Simplified Data Processing on
>   Large Clusters. OSDI, 2004.
> - Yuan Yu, Michael Isard, Dennis Fetterly, Mihai Budiu. DryadLINQ: A System for
>   General-Purpose Distributed Data-Parallel Computing Using a High-Level Language.
>   OSDI, 2008.
> 
> Автор: Michael Stonebraker

{{< param Summary >}}

Ряд публикаций Google об их крупномасштабных системах, включая Google File
System [^62], MapReduce, Chubby [^32] и BigTable [^37], возможно, самые
известные и влиятельные на рынке. Практически во всех случаях в этих новых
собственных системах реализовано небольшое подмножество функций обычных баз
данных, включая языки высокого уровня, оптимизаторы запросов и эффективные
стратегии выполнения. Однако эти системы и образовавшаяся экосистема Hadoop с
открытым исходным кодом оказались очень популярными у многих разработчиков.
Это привело к значительным инвестициям, маркетингу, исследовательскому интересу
и развитию этих платформ, которые сегодня постоянно меняются, но как экосистема
стали напоминать традиционные хранилища данных - с некоторыми важными
модификациями. Мы размышляем об этих тенденциях здесь.

## История и преемники {#history-and-successors}

Нашим первым чтением будет оригинальная статья Google MapReduce 2004 года.
MapReduce была библиотекой, созданной для упрощения параллельных распределенных
вычислений над распределенными данными в масштабе Google, в частности, для
пакетной перестройки индексов веб-поиска из просканированных страниц.
Маловероятно, что в то время традиционное хранилище данных могло справиться с
такой нагрузкой. Однако по сравнению с обычным хранилищем данных MapReduce
предоставляет интерфейс очень низкого уровня (двухэтапный поток данных,
two-stage dataflow), который тесно связан со стратегией отказоустойчивого
выполнения (промежуточная материализация между двухэтапным потоком данных).
Не менее важно, что MapReduce разрабатывался как библиотека для параллельного
программирования, а не как решение для комплексного хранилища данных; например,
MapReduce делегирует хранилище файловой системе Google (Google File System).
В то время члены сообщества баз данных осуждали данную архитектуру как упрощенную,
неэффективную и ограниченную в использовании [^53].

Хотя оригинальная статья о MapReduce была выпущена в 2003 году, до 2006 года,
когда Yahoo открыла исходный код реализации Hadoop MapReduce, не было практически
никакой активноcти вне Google. Впоследствии произошел взрыв интереса: в течение
года в разработке находился ряд проектов, включая Dryad (Microsoft) [^89], Hive
(Facebook) [^156], Pig (Yahoo) [^123]. Эти системы, которые мы будем называть
системами MapReduce, приобрели значительную популярность у разработчиков,
которые в основном были сконцентрированы в Кремниевой долине, а также привлекли
серьезные венчурные инвестиции. Множество исследований, охватывающих системы,
базы данных и сетевые сообщества, изучали вопросы, включая планирование,
устранение отставания, отказоустойчивость, оптимизацию запросов UDF и
альтернативные модели программирования [^16].

Практически сразу же пост-MapReduce системы расширили свой интерфейс и
функциональность, включив в них более сложные декларативные интерфейсы,
стратегии оптимизации запросов и эффективные среды выполнения. Сегодняшние
системы MapReduce начинают реализовывать всё большую часть набора функций
обычных СУБД. Последнее поколение движков обработки данных, таких как Spark
[^163], F1 [^143], Impala [^98], Tez [^1], Naiad [^119], Flink/Stratosphere [^9],
AsterixDB [^10] и Drill [^82] часто:
- предлагают языки запросов более высокого уровня, такие как SQL
- предлагают более продвинутые стратегии выполнения, включая возможность
  обработки общих графиков операторов
- используют индексы и другие функции источников структурированных входных
  данных (там, где возможно)
В экосистеме Hadoop механизмы потока данных стали основой для набора
высокоуровневых функций и декларативных интерфейсов, включая SQL [^15] [^156],
обработку графов [^64] [^110] и машинное обучение [^63] [^146].

Также растет интерес к функциональности потоковой обработки, пересматривая
многие концепции, впервые появившиеся в сообществе баз данных в 2000-х годах.
Растущие коммерческая и экосистема с открытым исходным кодом разработали
«соединители» для различных структурированных и полуструктурированных источников
данных, функций каталога (например, HCatalog), обслуживания данных и
ограниченных транзакционных возможностей (например, HBase). Большая часть этих
функций, таких как типичные оптимизаторы запросов в этих средах, являются
элементарными по сравнению со многими зрелыми коммерческими базами данных, но
быстро развиваются.

DryadLINQ, вторая выбранная статья для чтения этого раздела, возможно, наиболее
интересна своим интерфейсом: набором встроенных языковых привязок для обработки
данных, который легко интегрируется с Microsoft .NET LINQ для создания библиотеки
параллельных коллекций. DryadLINQ выполняет запросы через более раннюю систему
Dryad [^89], в которой реализована среда выполнения для произвольных графов
потока данных с использованием отказоустойчивости на основе воспроизведения.
Хотя DryadLINQ по-прежнему ограничивает программистов набором преобразований
наборов данных без побочных эффектов (включая «SQL-подобные» операции), он
предоставляет интерфейс значительно более высокого уровня, чем Map Reduce.
Языковая интеграция DryadLINQ, легкая отказоустойчивость и базовые методы
оптимизации запросов оказали влияние на более поздние системы обработки данных,
включая Apache Spark [^163] и Microsoft Naiad [^119].

## Вклад и наследение {#impact-and-legacy}

Существует по крайней мере три длительных воздействия явления MapReduce, которых
в противном случае могло бы не произойти. Эти идеи, как и сам распределенный
поток данных, не обязательно новы, но экосистема систем хранения и обработки
потоков данных после MapReduce значительно усилила их влияние:

1. *Гибкость схемы*. Возможно, самое важное, что традиционные хранилища
   данных - это «огороженные огороды»: загруженные данные являются чистыми,
   тщательно отобранными и имеют структуру. Напротив, системы MapReduce
   обрабатывают произвольно структурированные данные, чистые или грязные,
   тщательно отобранные или нет. Шаг загрузки отсутствует. Это означает, что
   пользователи могут сначала сохранить данные, а потом решить, что с ними
   делать. В сочетании с тем фактом, что хранение (например, в файловой системе
   Hadoop) значительно дешевле, чем в традиционном хранилище данных,
   пользователи могут позволить себе хранить данные все дольше и дольше. Это
   серьезный отход от традиционных хранилищ данных и ключевой фактор
   распространения и накопления «больших данных». Растущее число форматов
   хранения (например, Avro, Parquet, RCFile) объединяет полуструктурированные
   данные и результаты прогресса в хранении данных, например, поколоночное
   хранение. В отличие от XML, эта новейшая волна полуструктурированных данных
   еще более гибкая. В результате задачи извлечения-преобразования-загрузки
   (ETL) являются основной рабочей нагрузкой для пост-MapReduce движков. Трудно
   переоценить влияние гибкости схемы на современную практику управления данными
   на всех уровнях, от аналитика до программиста и поставщика аналитических
   услуг, и мы полагаем, что в будущем это станет еще более важным. Однако эта
   неоднородность не бесплатна: курировать такие «озера данных» стоит дорого
   (гораздо больше, чем хранение), и это тема, которую мы подробно рассмотрим в
   главе 12.
2. *Гибкость интерфейса*. Сегодня большинство пользователей взаимодействуют с
   движками «больших данных» на языках, подобных SQL. Однако, эти движки также
   позволяют пользователям программировать, используя комбинацию парадигм.
   Например, организация может использовать императивный код для выполнения
   синтаксического анализа файлов, SQL для проецирования столбцов и подпрограммы
   машинного обучения для кластеризации результатов - и все это в рамках одного
   инструмента. Тесная идиоматическая языковая интеграция, как в DryadLINQ,
   является обычным явлением, что еще больше улучшает программируемость. В то
   время как традиционные базы данных исторически поддерживали
   определяемые пользователем функции, интерфейсы этих новых движков упрощают
   выражение определяемых пользователем вычислений, а также упрощают интеграцию
   результатов определяемых пользователем вычислений с результатами запросов,
   выраженными с использованием традиционных реляционных конструкций, таких как
   SQL. Гибкость интерфейса и интеграция являются сильным аргументом в пользу
   предложений по аналитике данных; возможность сочетать ETL, аналитику и
   постобработку в единой системе чрезвычайно удобна для программистов, но не
   обязательно для пользователей традиционных инструментов бизнес-аналитики,
   которые используют традиционные интерфейсы JDBC.
3. *Архитектурная гибкость*. Общая критика РСУБД заключается в том, что их
   архитектура слишком тесно связана: хранение, обработка запросов, управление
   памятью, обработка транзакций и прочие тесно взаимосвязаны, при этом на
   практике между ними отсутствует четкий интерфейс. Напротив, в результате
   восходящего развития экосистема Hadoop фактически построила хранилище данных
   в виде серии модулей. Сегодня организации могут писать и запускать программы
   в сырой файловой системе (например, HDFS), в любом количестве движков
   потока данных (например, Spark), используя пакеты расширенной аналитики
   (например, GraphLab [^105], Parameter Server [^101]) или или через SQL
   (например, Impala [^98]). Эта гибкость увеличивает накладные расходы на
   производительность, но возможность смешивать и сопоставлять компоненты и
   аналитические пакеты беспрецедентна в этом масштабе. Эта архитектурная
   гибкость, пожалуй, наиболее интересна разработчикам систем и поставщикам, у
   которых есть дополнительные степени свободы при проектировании своих
   инфраструктурных предложений.

Подводя итог, можно сказать, что доминирующей темой в современной инфраструктуре
управления распределенными данными является гибкость и неоднородность: форматов
хранения, парадигм вычислений и реализаций систем. Из них неоднородность формата
хранения, вероятно, оказывает наибольшее влияние на порядок или даже больше
просто потому, что она одинаково влияет как на новичков, так и на экспертов и
архитекторов. Напротив, неоднородность вычислительных парадигм больше всего
влияет на экспертов и архитекторов, в то время как неоднородность реализации
систем больше всего влияет на архитекторов. Все три являются актуальными и
захватывающими разработками для исследования баз данных, с нерешенными вопросами
о влиянии на рынок и долговечности.

## Заглядывая вперёд {#looking-ahead}

В некотором смысле, идея MapReduce была недолговечной, экстремальной архитектурой,
которая открыла пространство для дизайна. Архитектура была простой и хорошо
масштабируемой, а её успех в области открытого исходного кода заставил многих
осознать, что существует спрос на альтернативные решения и воплощенный в них
принцип гибкости (не говоря уже о рыночных возможностях для более дешевых
решений для хранилищ данных, основанных на открытых решениях). Возникший в
результате интерес по-прежнему удивляет многих и объясняется многими факторами,
включая дух времени сообщества, грамотный маркетинг, экономику и технологические
сдвиги. Интересно рассмотреть, какие различия между этими новыми системами и
СУБД являются фундаментальными, а какие обусловлены инженерными улучшениями.

Сегодня всё ещё ведутся споры о подходящей архитектуре для масштабируемой
обработки данных. Например, Расмуссен и др. даюти веский аргумент в пользу
того, почему промежуточная отказоустойчивость (*intermediate fault tolerance*)
не нужна, за исключением очень больших (более 100 узлов) кластеров [^132].
В качестве другого примера, McSherry и др. красочно проиллюстрировали, что
многие рабочие нагрузки могут быть эффективно обработаны с использованием одного
сервера (или потока!), что вообще устраняет необходимость в распределении [^113].
Недавно в таких системах, как проект GraphLab [^105] было высказано
предположение, что предметно-ориентированные системы необходимы для повышения
производительности; более поздние работы, в том числе Grail [^58] и GraphX [^64],
опровергли это предположение. Следующая волна недавних предложений также
предлагала новые интерфейсы и системы для потоковой обработки (*stream
processing*), обработки графов, асинхронного программирования и
универсального машинного обучения. Действительно ли нужны эти специализированные
системы, или все они могут управляться одной аналитической машиной? Время
покажет, но я чувствую толчок к консолидации.

Наконец, было бы упущением не упомянуть Spark, которому всего шесть лет, но
который становится все более популярным среди разработчиков и очень хорошо
поддерживается как стартапами, поддерживаемыми венчурным капиталом (например,
Databricks), так и такими авторитетными фирмами, как Cloudera и IBM. Хотя мы
включили DryadLINQ в качестве пост-MapReduce примера из-за её
исторической значимости и технической глубины, статья Spark [^163], написанная в
первые дни проекта, и недавние расширения, включая SparkSQL [^15], заслуживают
внимания. Как и Hadoop, Spark привлек большой интерес на относительно ранней
стадии развития. Сегодня Spark еще предстоит пройти путь до того, как его набор
функций станет конкурентом традиционного хранилища данных. Однако его набор
функций быстро растет, и ожидания от Spark как преемника MapReduce в экосистеме
Hadoop высоки; например, Cloudera работает над заменой MapReduce на Spark в
экосистеме Hadoop [^81]. Время покажет, верны ли эти ожидания; между тем, разрыв
между традиционными хранилищами и системами, появившимися после MapReduce,
быстро сокращается, в результате чего создаются системы, которые не только
хороши в хранилищах данных, но и в гораздо больших областях.


[^1]: Apache Tez. https://tez.apache.org/.

[^9]: A. Alexandrov, R. Bergmann, S. Ewen, J.-C. Freytag, F. Hueske, A. Heise, O.
Kao, M. Leich, U. Leser, V. Markl, et al. The Stratosphere platform for big
data analytics. The VLDB Journal, 23(6):939–964, 2014.

[^10]: S. Alsubaiee, Y. Altowim, H. Altwaijry, A. Behm, V. Borkar, Y. Bu, M.
Carey, I. Cetindil, M. Cheelangi, K. Faraaz, et al. Asterixdb: A scalable,
open source bdms. In VLDB, 2014.

[^15]: M. Armbrust, R. S. Xin, C. Lian, Y. Huai, D. Liu, J. K. Bradley, X. Meng,
T. Kaftan, M. J. Franklin, A. Ghodsi, et al. Spark SQL: Relational data
processing in spark. In SIGMOD, 2015.

[^16]: S. Babu and H. Herodotou. Massively parallel databases and MapReduce
systems. Foundations and Trends in Databases, 5(1):1–104, 2013.

[^32]: M. Burrows. The chubby lock service for loosely-coupled distributed
systems. In OSDI, 2006.

[^37]: F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach, M. Burrows,
T. Chandra, A. Fikes, and R. E.  Gruber. Bigtable: A distributed storage
system for structured data. In OSDI, 2006.

[^53]: D. DeWitt and M. Stonebraker. Mapreduce: A major step backwards. The
Database Column, 2008.

[^58]: J. Fan, A. Gerald, S. Raj, and J. M. Patel. The case against specialized
graph analytics engines. In CIDR, 2015.

[^62]: S. Ghemawat, H. Gobioff, and S.-T. Leung. The google file system. In
SOSP, 2003.

[^63]: A. Ghoting, R. Krishnamurthy, E. Pednault, B. Reinwald, V. Sindhwani, S.
Tatikonda, Y. Tian, and S. Vaithyanathan. Systemml: Declarative machine
learning on mapreduce. In ICDE, 2011.

[^64]: J. E. Gonzales, R. S. Xin, D. Crankshaw, A. Dave, M. J. Franklin, and I.
Stoica. Graphx: Unifying data-parallel and graph-parallel analytics. In OSDI, 2014.

[^81]: D. Harris. Forbes: Why Cloudera is saying ’Goodbye, MapReduce’ and
’Hello, Spark’, 2015. http://fortune.com/2015/09/09/cloudera-spark-mapreduce/.

[^82]: M. Hausenblas and J. Nadeau. Apache Drill: Interactive ad-hoc analysis at
scale. Big Data, 1(2):100–104, 2013.

[^89]: M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly. Dryad:
distributed data-parallel programs from sequential building blocks. In EuroSys, 2007.

[^98]: M. Kornacker, A. Behm, V. Bittorf, T. Bobrovytsky, C. Ching, A. Choi, J.
Erickson, M. Grund, D. Hecht, M. Jacobs, et al. Impala: A modern, open-source
sql engine for hadoop. In CIDR, 2015.

[^101]: M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski,
J. Long, E. J. Shekita, and B.-Y. Su. Scaling distributed machine learning with
the parameter server. In OSDI, 2014.

[^105]: Y. Low, D. Bickson, J. Gonzalez, C. Guestrin, A. Kyrola, and J. M.
Hellerstein. Distributed graphlab: a framework for machine learning and data
mining in the cloud. In VLDB, 2012.

[^110]: G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn, N.
Leiser, and G. Czajkowski. Pregel: a system for large-scale graph processing.
In SIGMOD, 2010.

[^113]: F. McSherry, M. Isard, and D. G. Murray. Scalability! But at what COST?
In HotOS, 2015.

[^119]: D. G. Murray, F. McSherry, R. Isaacs, M. Isard, P. Barham, and M. Abadi.
Naiad: A timely dataflow system.  In SOSP, 2013.

[^123]: C. Olston, B. Reed, U. Srivastava, R. Kumar, and A. Tomkins. Pig latin:
a not-so-foreign language for data processing. In SIGMOD, 2008.

[^132]: A. Rasmussen, V. T. Lam, M. Conley, G. Porter, R. Kapoor, and A. Vahdat.
Themis: An i/o-efficient mapreduce. In SoCC, 2012.

[^143]: J. Shute, R. Vingralek, B. Samwel, B. Handy, C. Whipkey, E. Rollins, M.
Oancea, K. Littlefield, D. Menestrina, S. Ellner, et al. F1: A distributed sql
database that scales. In VLDB, 2013.

[^146]: E. R. Sparks, A. Talwalkar, V. Smith, J. Kottalam, X. Pan, J. Gonzalez,
M. J. Franklin, M. Jordan, T. Kraska, et al. Mli: An api for distributed
machine learning. In ICDM, 2013.

[^156]: A. Thusoo, J. S. Sarma, N. Jain, Z. Shao, P. Chakka, S. Anthony, H. Liu,
P. Wyckoff, and R. Murthy. Hive: A warehousing solution over a map-reduce
framework. In VLDB, 2009.

[^163]: M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauley, M. J.
Franklin, S. Shenker, and I. Stoica.  Resilient distributed datasets: A
fault-tolerant abstraction for in-memory cluster computing. In NSDI, 2012.
